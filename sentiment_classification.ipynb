{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251589f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb63b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    # 데이터의 중복 제거\n",
    "    train_data = list(set(train_data))\n",
    "    test_data = list(set(test_data))\n",
    "    \n",
    "    # NaN 결측치 제거\n",
    "    train_data = [data for data in train_data if str(data) != 'nan']\n",
    "    test_data = [data for data in test_data if str(data) != 'nan']\n",
    "    \n",
    "    # 토큰화 및 불용어 제거\n",
    "    X_train = [remove_stopwords(tokenizer.morphs(sentence)) for sentence in train_data]\n",
    "    X_test = [remove_stopwords(tokenizer.morphs(sentence)) for sentence in test_data]\n",
    "    \n",
    "    # 단어 집합 생성\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    if num_words is None:\n",
    "        counter = counter.most_common(num_words)\n",
    "    else:\n",
    "        counter = counter.items()\n",
    "    word_to_index = {word: index + 2 for index, (word, _) in enumerate(counter)}\n",
    "    word_to_index['<PAD>'] = 0  # 패딩을 위한 토큰\n",
    "    word_to_index['<UNK>'] = 1  # OOV(out-of-vocabulary)를 위한 토큰\n",
    "    \n",
    "    # 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "    X_train = get_encoded_sentences(X_train, word_to_index)\n",
    "    X_test = get_encoded_sentences(X_test, word_to_index)\n",
    "    \n",
    "    # 레이블 생성\n",
    "    y_train = np.array([0] * len(X_train))\n",
    "    y_test = np.array([0] * len(X_test))\n",
    "    \n",
    "    # 문장 길이 분포 확인\n",
    "    total_data = X_train + X_test\n",
    "    num_tokens = [len(tokens) for tokens in total_data]\n",
    "    max_len = np.percentile(num_tokens, 95)  # 상위 95%의 문장 길이를 최대 문장 길이로 설정\n",
    "    mean_len = np.mean(num_tokens)\n",
    "    \n",
    "    max_len = int(max_len)\n",
    "    mean_len = int(mean_len)\n",
    "    \n",
    "    # 패딩 추가\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len, dtype=object)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len, dtype=object)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, word_to_index, max_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0691b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_122/203380902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 예시 실행 코드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# train_data와 test_data는 각각 학습 데이터와 테스트 데이터로 구성된 리스트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 문장 길이 분포 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_122/3407639104.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(train_data, test_data, num_words)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 텍스트 스트링을 사전 인덱스 스트링으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_122/203380902.py\u001b[0m in \u001b[0;36mget_encoded_sentences\u001b[0;34m(sentences, word_to_index)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# def get_encoded_sentences(sentences, word_to_index):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_122/203380902.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_encoded_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# def get_encoded_sentences(sentences, word_to_index):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [word_to_index['<PAD>']] + [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentences]\n",
    "\n",
    "# def get_encoded_sentences(sentences, word_to_index):\n",
    "#     return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "def plot_sentence_length_distribution(data):\n",
    "    num_tokens = [len(tokens) for tokens in data]\n",
    "    print(num_tokens)\n",
    "    plt.hist(num_tokens, bins=100)\n",
    "    plt.xlabel('Length of Sentences')\n",
    "    plt.ylabel('Number of Sentences')\n",
    "    plt.show()\n",
    "\n",
    "# 예시 실행 코드\n",
    "# train_data와 test_data는 각각 학습 데이터와 테스트 데이터로 구성된 리스트\n",
    "X_train, y_train, X_test, y_test, word_to_index, max_len = load_data(train_data, test_data, num_words=10000)\n",
    "\n",
    "# 문장 길이 분포 확인\n",
    "plot_sentence_length_distribution(X_train)\n",
    "\n",
    "# 모델 구성 및 validation set 구성\n",
    "# 모델 훈련 개시\n",
    "# Loss, Accuracy 그래프 시각화\n",
    "# 학습된 Embedding 레이어 분석\n",
    "# 한국어 Word2Vec 임베딩 활용하여 성능 개선\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_length_distribution(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1598d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
